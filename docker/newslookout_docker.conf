# This is the configuration file for the NewsLookout web scraping application.
#
# It is organised into 4 sections: installation, operation, logging and plugins.
#
##################################################################################################
#                                                                                                #
# Notice:                                                                                        #
# This software is intended for demonstration and educational purposes only. This software is    #
# experimental and a work in progress. Under no circumstances should these files be used in      #
# relation to any critical system(s). Use of these files is at your own risk.                    #
#                                                                                                #
# Before using it for web scraping any website, always consult that website's terms of use.      #
# Do not use this software to fetch any data from any website that has forbidden use of web      #
# scraping or similar mechanisms, or violates its terms of use in any other way. The author is   #
# not liable for such kind of inappropriate use of this software.                                #
#                                                                                                #
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,            #
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR       #
# PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE      #
# FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR           #
# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER         #
# DEALINGS IN THE SOFTWARE.                                                                      #
#                                                                                                #
##################################################################################################


[installation]
# this section lists the file and directory locations

# A common variable can be re-used in subsequent variables
# Here, the installation directory is specified as the 'prefix' variable:
prefix = /opt/newslookout

# The configuration directory is specified relative to the prefix director
# However, an absolute path could also be given:
conf_dir = /etc/newslookout

# The data_dir specifies the name of the data directory where the data files will be saved:
data_dir =  /var/cache/newslookout_data

# the master data files will be stored in this folder
master_data_dir = %(data_dir)s/master_data

# location of the log file, here it is specified relative to the data directory:
log_file = /var/log/newslookout/newslookout.log

# location of the plugins directory
plugins_dir = /opt/newslookout/plugins

# location of the contributed plugins
plugins_contributed_dir = /opt/newslookout/plugins_contrib

# If a PID (process identifier) file exists, the application will not launch.
# This is to prevent multiple instances being launched on the same machine.
# As part of the shutdown sequence, the PID file will be automatically deleted.
pid_file = /var/run/newslookout/newslookout.pid

# the sqlite data file that stores the history of previously retrieved URLs
completed_urls_datafile = %(data_dir)s/completed_urls.db

cookie_file=%(data_dir)s/cookies.txt


[operation]
# levels of recursion to follow links for identifying news articles within websites
# min value is 1, max is 4, any other values do not have any effect
recursion_level=2

# time in seconds to wait before refreshing the progress bar:
progressbar_refresh_interval=15

# time in seconds to wait for when retrieving a page:
fetch_timeout = 60

# time to wait to establish TCP connection:
connect_timeout = 10

# number of times to retry connecting if failed
retry_count = 3

# the fixed number of seconds to wait between web fetches, this
# fixed time is added to the random time to determine the total wait time
# between two web fetches to the same URL 
retry_wait_sec = 3

# minimum number of seconds to wait when calculating the random wait time
retry_wait_rand_min_sec = 2

# maximum number of seconds to wait when calculating the random wait time
retry_wait_rand_max_sec = 5

# should raw html be saved as compressed bzipped files?
save_html=True
#save_html=False


# proxy configuration parameters, for example:
# proxy_url_http=127.0.0.1:8080
# proxy_url_https=127.0.0.1:8080
proxy_url_http=""
proxy_url_https=""

# user authentication to the proxy, username and password are decoded before converting to plain text:
proxy_user=""
proxy_password=""

# proxy certificate path for the organisation's proxy server, this is to be trusted
# If proxy_ca_certfile parameter is set to a directory, it must have been processed using the c_rehash utility supplied with OpenSSL.
proxy_ca_certfile=""

# WARNING: set parameter verify_ca_cert to False only if a cert cannot be verified since it may be internal to an organisation.
# if it is set to False, requests will accept any TLS certificate presented by the server, and will ignore hostname mismatches and/or expired certificates, which will make the application vulnerable to man-in-the-middle (MitM) attacks.
verify_ca_cert=True

# the user agents to use for the web scraper's HTTP(S) requests:
# use pipe delimiter to specify multiple different user agents
# these will be rotated in round robin manner with each subsequent request.
user_agent=Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)|Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14|Mozilla/5.0 (PlayStation 4 3.11) AppleWebKit/537.73 (KHTML, like Gecko)|Mozilla/5.0 (Windows NT 10.0; WOW64; rv:77.0) Gecko/20100101 Firefox/77.0|Mozilla/5.0 (X11) AppleWebKit/62.41 (KHTML, like Gecko) Edge/17.10859 Safari/452.6|Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko|FeedFetcher-Google; (+http://www.google.com/feedfetcher.html)|Opera/9.80 (S60; SymbOS; Opera Mobi/SYB-1107071606; U; en) Presto/2.8.149 Version/11.10


[logging]
# log levels can be one of the following
# (starting from less verbosity to full verbosity): ERROR, WARN, INFO, or DEBUG
# log_level=ERROR
# log_level=WARN
log_level=INFO
# log_level=DEBUG

# max file size of the log file, when the log file grows
# beyond this size limit, it will be rotated:
max_logfile_size=2048000

# this many backups of the log file will be retained
logfile_backup_count=30


[plugins]
# In this section, list the names of all modules to be enabled:
# the number after the pipe character '|' indicates execution priority,
# A lower number indicates higher priority, so it will be run before others are run
plugin01=mod_in_gdelt|1
plugin02=mod_in_nse|2
plugin03=mod_in_bse|2
plugin04=mod_en_in_inexp_business|3
plugin05=mod_en_in_ndtv|3
plugin06=mod_en_in_business_std|3
plugin07=mod_en_in_livemint|3
plugin08=mod_en_in_timesofindia|3
plugin09=mod_en_in_moneycontrol|3
plugin10=mod_en_in_hindu|3
plugin11=mod_en_in_indiakanoon|5
plugin12=mod_en_in_trak|4
plugin13=mod_en_in_forbes|4
plugin14=mod_en_in_ecotimes|3
plugin15=mod_dataprep|1
plugin16=mod_keywordflags|3
# it is recommended to keep these two plugins disabled, they will slow down the entire application
# since the models are very large and computationally intensive
# unless you are running the application on very capable hardware:
#plugin17=mod_dedupe|2
#plugin18=mod_eventclass|4
plugin19=mod_solrsubmit|5

# --- Put configuration parameters specific to each plugin here ---
# As a good practice, to avoid mixing up config names, prefix each config parameter with the plugin name,

# for the SOLR plugin, put the SOLR engine host/port and user info:
mod_solrsubmit_solr_host_port="https://127.0.0.1:3839"
mod_solrsubmit_username="solr"

# For the spacy model used for the dedupe plugin:
# install the model as follows: python -m spacy download en_core_web_lg
mod_dedupe_spacymodel=en_core_web_lg

# For the news event tone classification model - finBERT:
# Download the models from:
# https://gohkust-my.sharepoint.com/:f:/g/personal/imyiyang_ust_hk/EksJcamJpclJlbMweFfB5DQB1XrsxURYN5GSqZw3jmSeSw?e=KAyhsX
mod_eventclass_modelfile=/opt/newslookout/models/bert_models/pretrained_weights/pytorch_model.bin
# save the model file and the config.json file to this folder:
mod_eventclass_weightspath=/opt/newslookout/models/bert_models/pretrained_weights
# obtain the vocabulary file from:
# https://gohkust-my.sharepoint.com/:t:/g/personal/imyiyang_ust_hk/EX3C-KM9bTxOjdttsPslLZUBw_mh9Jdh8PB0WTv6b2tEIA?e=DYBVJY
mod_eventclass_vocab_path=/opt/newslookout/models/bert_models/finbert_vocab


## end of file ##
